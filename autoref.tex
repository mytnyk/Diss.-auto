% ------------------------------------------------------------------------
% Ph.D. Autoreferat ++ Oleg, 2008 ************
% ------------------------------------------------------------------------

\documentclass[12pt,a4paper,twoside]{article}
\usepackage[cp1251]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[ukrainian]{babel}
\usepackage[centertags]{amsmath}
\usepackage{amsfonts, amssymb, amsthm}
\usepackage{../Shared.TEX/phdauto}
\usepackage{color,url}
\usepackage{graphicx}
\usepackage{pscyr}
\usepackage{fancyhdr}
\usepackage{../Shared.TEX/algorithmic}
%\usepackage[unicode,colorlinks=true,linkcolor=black,citecolor=red,urlcolor=blue]{hyperref}

% 10pt one page printing
% A5 148x210 mm  5.83" 8.27"
\oddsidemargin  = -8mm% (17, 15) 25mm + 116mm = 148mm
\evensidemargin = -10mm% (15, 17)
\topmargin     =-15mm%
\textheight    =175mm%  1 in = 72.27 pt 1mm =2.85pt
\textwidth     =116mm%
%\headheight =0mm%
\headsep =3mm%
\footskip =0mm%

% 11pt two page printing (13,2,10,10)
% A5 148x210 mm  5.83" 8.27"
\oddsidemargin  = -10mm%
\evensidemargin = -23mm%
\topmargin     =-25mm%
\textheight    =197mm%
\textwidth     =131mm%
%\headheight =0mm%
\headsep =3mm%
\footskip =0mm%

% A4 210x297 mm
%\oddsidemargin  = -5mm% 25mm - 5mm = 20mm; 210-175=20+15mm
% FOR PRINTING
\oddsidemargin  = -7mm%!!!
\evensidemargin = -10mm% 25mm - 10mm = 15mm;
\topmargin     =-15mm% 25mm - 15mm = 10mm;
\textheight    =262mm%
\textwidth     =175mm%
%\headheight =0mm%
\headsep =5mm%
\footskip =0mm%

\tolerance = 300    % міра розрідженості строки
\hyphenpenalty = 300% частота переносів
\parindent = 10mm % абзацний відступ
\fontfamily{cmr}%
\raggedbottom % можна робити сторінки різної довжини
\pagestyle{fancy} \lfoot{} \cfoot{} \rfoot{} \lhead{}
%\chead{---~\thepage~---} \rhead{} \renewcommand{\headrulewidth}{0pt}
\chead{~\thepage~} \rhead{} \renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}

\hyphenation{ма-те-ма-тич-них вхід-них век-тору ус-піш-но
еко-но-міч-них ви-ключ-но ем-пі-рич-них тех-ніч-но-го
гі-пер-па-ра-мет-рів склад-нос-ті не-чіт-ких ней-ро-не-чіт-ких
ба-ри-цен-трич-них сис-тем-но-го по-ло-жен-ня ви-би-ра-єть-ся
при-пус-ка-ю-чи ха-рак-те-рис-тич-но-му по-род-жує під-тверд-жен-ня
прог-ноз зглад-же-ної сис-те-ма-тич-на кос-міч-них вис-но-вок
век-то-рів ви-ко-рис-та-но sto-chas-tic}

\begin{document}

\renewcommand{\tablename}{Таблиця}

%\fontsize{10}{12}\selectfont %cmr, cmss, cmtt
%\fontsize{11}{14}\selectfont %for 2 page printing
\fontsize{14}{17.8}\selectfont %a4
%\fontsize{14.4}{18}\selectfont %a4

% cover
\thispagestyle{empty}
\vspace{10mm}
 \begin{center}
{\large\bf НАЦІОНАЛЬНИЙ ТЕХНІЧНИЙ УНІВЕРСИТЕТ УКРАЇНИ} \\
{\bf ``КИЇВСЬКИЙ ПОЛІТЕХНІЧНИЙ ІНСТИТУТ''}
\end{center}

\vspace{20mm}
\begin{center}
{\bf МИТНИК ОЛЕГ ЮРІЙОВИЧ}
\end{center}

\vspace{15mm} %
%\hspace{75mm}
%\parbox[t]{75mm}{
%УДК 004.855:[519.226+519.853]}
\hspace{85mm}
\parbox[t]{65mm}{УДК 004.855:681.518}

% 004.855 Обучение и индуктивный вывод
% 519.226. Теория статистических выводов и решений
% 519.853 Нелинейное программирование

% 681.51 Системы автоматического управления (САУ). Кибернетические
%       характеристики систем. Системы по принципу действия

% 681.518 Информационные системы (в автоматическом управлении)

\vspace{10mm}
\begin{center}

%{\large\bf { ПРОГНОЗУВАННЯ СТОХАСТИЧНИХ ПРОЦЕСІВ \\НА ОСНОВІ
%БАЙЄСІВСЬКИХ МОДЕЛЕЙ РЕГРЕСІЇ \\У ФОРМІ БЕРНШТЕЙНА}}

%{\large\bf { ПРОГНОЗУВАННЯ СТОХАСТИЧНИХ ПРОЦЕСІВ \\НА ОСНОВІ
%ІНФОРМАЦІЙНОЇ ТЕХНОЛОГІЇ СИНТЕЗУ НЕЧІТКИХ БАЗ ЗНАНЬ
%\\ПОНИЖЕНОЇ СКЛАДНОСТІ}}

%{\large\bf { ГІБРИДНА ІНТЕЛЕКТУАЛЬНА ІНФОРМАЦІЙНА ТЕХНОЛОГІЯ
%\\ СИНТЕЗУ МОДЕЛЕЙ СТОХАСТИЧНИХ ПРОЦЕСІВ }}

%{\large\bf { ПРОГНОЗУВАННЯ СТОХАСТИЧНИХ ПРОЦЕСІВ \\НА ОСНОВІ
%ГІБРИДНИХ ІНФОРМАЦІЙНИХ ТЕХНОЛОГІЙ}}

%{\large\bf { ГІБРИДНІ ІНФОРМАЦІЙНІ ТЕХНОЛОГІЇ І МЕТОДИ
%\\ СИНТЕЗУ МОДЕЛЕЙ СТОХАСТИЧНИХ ПРОЦЕСІВ }}

{\large\bf { ІНФОРМАЦІЙНІ ТЕХНОЛОГІЇ СИНТЕЗУ РОБАСТНИХ \\
НЕЙРОНЕЧІТКИХ МОДЕЛЕЙ СТОХАСТИЧНИХ ПРОЦЕСІВ }}


\vspace{35mm} {\bf 05.13.06. --- Інформаційні технології}

\vspace{30mm} {\bf
АВТОРЕФЕРАТ\\
дисертації на здобуття наукового ступеня\\
кандидата технічних наук}
\end{center}

\vspace{15mm} %

\vfill \centerline{\bf Київ -- 2008} \vspace{10mm}
\newpage
% cover underside
\thispagestyle{empty}

\noindent Дисертацією є рукопис.

\vspace{5mm}

\noindent Робота виконана в навчально-науковому комплексі ``Інститут
прикладного системного аналізу'' Національного технічного
університету України ``Київський політехнічний інститут''.

\vspace{5mm}

\noindent
\begin{tabular}{lp{120mm}}
Науковий керівник: & доктор технічних наук, професор \\
 & Бідюк Петро Іванович, \\
 & Навчально-науковий комплекс ``Інститут прикладного системного аналізу''
 Національного технічного університету України ``Київський політехнічний інститут'',\\
 & провідний науковий співробітник відділу математичних методів системного аналізу.\\
  \\
Офіційні опоненти:
 & доктор фізико-математичних наук, професор \\
 & Крак Юрій Васильович,\\
 & Київський національний університет імені Тараса Шевченка,\\
 & професор кафедри моделювання складних систем;\\
 \\
 & доктор технічних наук, професор \\
 & Томашевський Валентин Миколайович,\\
 & Національний технічний університет України ``Київський політехнічний інститут'',\\
 & професор кафедри автоматизованих систем обробки інформації та управління.\\
 \\
\end{tabular}

\vfill

\noindent Захист відбудеться \hspace{40mm} 2008 р. о \hspace{20mm}
годині на засіданні спеціалізованої вченої ради Д26.002.03 в
Національному технічному університеті України ``Київський
політехнічний інститут'' за адресою: 03056, Київ, просп.
Перемоги~37, корп.~35, ауд.~006.

\vspace{5mm}

\noindent З дисертацією можна ознайомитись у бібліотеці
Національного технічного університету України ``КПІ''.

\vspace{5mm}

\noindent Автореферат розісланий \hspace{40mm} 2008р.

\vspace{5mm}

\noindent Вчений секретар

\noindent спеціалізованої вченої ради

\noindent д.т.н., професор \hspace{90mm} Новіков О.М.


\newpage
\setcounter{page}{1}

\section*{ЗАГАЛЬНА ХАРАКТЕРИСТИКА РОБОТИ}

{\bf Актуальність теми.} Прогнозування поведінки технічних процесів
успішно виконується на основі диференційних рівнянь. Тоді як
прогнозування економічних, екологічних, соціальних процесів або так
зване інтелектуальне прогнозування виконується на основі правил,
оскільки досвідчені менеджери приймають ефективні рішення на рівні
практичних міркувань. Для побудови інтелектуального прогнозу
доцільно використовувати експертні системи на основі нечітких баз
знань. Такі бази знань використовують апарат нечіткої логіки, сила
якого полягає у здатності створювати кількісне представлення для
лінгвістичних змінних, а також ефективно відображати залежності між
цими змінними у вигляді нечітких правил. Однак нечіткі бази знань,
які побудовані виключно на основі експертної інформації, є
суб'єктивними і повністю залежать від кваліфікації експертів. Тому
необхідно підкріплювати висновки і рішення експертів технічним
аналізом процесів. Для цього, після побудови апріорної моделі
процесу у формі нечіткої бази знань, проводять навчання цієї моделі
на емпіричних даних. Здатність навчатися на даних притаманна
нейроматематичним технологіям. Тож гібридизація нечітких та
нейроматематичних технологій в так звані нейронечіткі технології є
важливим інструментом інтелектуального прогнозування. Нейронечіткі
технології дозволяють перетворювати апріорні нечіткі правила в
аналітичні моделі, навчати ці моделі, і перетворювати їх знову в
нечіткі правила, які обґрунтовані технічним аналізом. Проблема
навчання моделей процесів з людським фактором це проблема
відновлення стохастичних залежностей. Тож дисертаційна робота
присвячена дослідженню некоректних за Адамаром задач відновлення
стохастичних залежностей на основі емпіричних даних, побудові
нейронечітких моделей за допомогою інтелектуальних інформаційних
технологій, та практичному застосуванню нових моделей для синтезу
нечітких баз знань та прогнозування стохастичних процесів.

Можна виділити три основні групи інформаційних технологій синтезу
моделей стохастичних процесів та інтелектуального аналізу даних. З
точки зору теорії суб'єктивної ймовірності --- байєсівські
технології. З точки зору теорії статистичного навчання ---
технологія опорних векторів. З точки зору теорії адаптивного
моделювання --- нейронечіткі технології.
%Pierre Simon Laplace.Bayes.Savage.Jaynes.Gull.Mackay
%Томас Байєс, Леонард Джимі Севідж, Едвін Джейнс, Стефан Гал, Девід Маккей, Чарльз Елкан
%Говард Райфа, Джон М. Кейнс, сір Гарольд Джефрі, Френк Пламптон Рамсей, Абрахам Вальд
Сучасні байєсівські технології і так званий байєсівський індуктивний
висновок ґрунтується на теорії суб'єктивної ймовірності. Історія
становлення байєсівської школи, яка розглядає ймовірність як ступінь
впевненості по відношенню до випадкової події, пов'язана з іменами
Л.Дж.Севіджа (L.J.Savage), Дж.Кейнса, Г.Джефрі, Ф.Рамсея, А.Вальда.
Подальші дослідження Е.Джейнса (E.Jaynes) в цьому напрямку
стосуються проблеми ``об'єктивного'' вибору апріорних ймовірностей
байєсівського висновку на основі принципу максимальної ентропії.
Байєсівський підхід до ідентифікації систем знайшов своє продовження
в роботах С.Гала (S.Gull), де вперше була сформульована концепція
байєсівського підтвердження параметрів апріорних розподілів для
задач регресійного аналізу. Потім Д.Маккей (D.Mackay) використав і
розвинув байєсівський принцип підтвердження з метою регуляризації
штучних нейронних мереж. В наш час байєсівські технології активно
використовуються пошуковими системами мережі Інтернет та системами
фільтрації електронної пошти.

%індукція – спосіб мислення, що йде від часткової до загальної
%закономірності

%Bayesian inference is a statistical inference in which
%probabilities are interpreted not as frequencies or proportions or
%the like, but rather as degrees of belief.

%Bernhard Boser, Isabelle Guyon, Corinna Cortes
%Alexander Smola, Federico Girosi, Steven Golowich, Bernhard Scholkopf, Christopher Burges,
%Steve Gunn, Klaus-Robert Muller, Michael Tipping, Martin Law, James Kwok, Sathiya Keerthi
%Matthias Seeger, Robert Williamson, Peter Bartlett
%Malte Kuss, Carl Edward Rasmussen, Christopher Williams

Початком принципово іншого підходу до аналізу процесів стала
статистична теорія навчання В.Н.Вапника, яка намагається пояснити
процес навчання зі статистичної точки зору, вивчає проблеми
індуктивного висновку та прогнозування. В рамках цієї теорії
сформульований принцип мінімізації структурного ризику, який став
передумовою створення машини опорних векторів для задач
класифікації. Метод опорних векторів був поширений А.Смолою
(A.Smola) та Б.Шолькопфом (B.Sch\"{o}lkopf) на задачі побудови
робастних регресійних моделей і розроблена так звана регресія
опорних векторів (РОВ). Подальші дослідження пов'язані в основному з
оптимальним вибором гіперпараметрів РОВ, які визначають
характеристики шуму. Так, використавши байєсівську індукцію Маккея
для знаходження гіперпараметрів, М.Лоу (M.Law) та Дж.Квок (J.Kwok)
започаткували байєсівську РОВ. Питання про вибір ефективного
характеристичного
%\footnote{Ідея методу опорних векторів полягає в
%побудові розділяючої гіперплощини в деякому просторі. Цей простір
%називають спрямляючим, характеристичним, або простором ознак (англ.
%feature space).}
простору для РОВ залишається відкритим.

%Warren S. McCulloch, Walter Pitts, Lotfi Asker Zadeh, Ebrahim Mamdani
%Tomohiro Takagi, Michio Sugeno, John Koza, Janos Madar
Нейротехнології беруть свій початок з фундаментальної роботи
В.Мак\-Каллока і В.Піттса в області моделювання нервової системи.
Саме з цієї роботи почався розвиток штучного інтелекту та нейронних
мереж, який пов'язаний з іменами В.М.Глушкова, %М.З.Згуровського,
О.А.Павлова, О.Г.Івахненка, Н.Віне\-ра, Ф.Розенблата, Д.Румельхарта
та Дж.Хопфілда. Новим етапом розвитку штучного інтелекту стала
концепція нечіткої логіки, запропонована Л.Заде та розвинута
Е.Мамдані, Т.Такаґі, М.Суґено для адаптивних нечітких контролерів.
Зв'язок між нейронними моделями і нечіткою логікою вперше було
встановлено М.Брауном (M.Brown) і К.Харрісом (C.Harris) в так званих
нейронечітких адаптивних мережах, які володіють лінгвістичною
прозорістю нечіткої логіки в поєднанні з аналітичною зручністю
нейронних моделей. Типовими представниками таких нейронечітких
архітектур стали система адаптивного нечіткого виводу (ANFIS)
Р.Джанга (R.Jang) та адаптивний сплайновий алгоритм моделювання
(ASMOD) Т.Кавлі (T.Kavli). Оскільки нейронечіткі мережі потерпають
від прокляття вимірності, то подальші дослідження спрямовані на
вирішення саме цієї проблеми. Зокрема, К.Харріс і К.Хонг (X.Hong)
запропонували нейронечіткий алгоритм моделювання поліноміальної
складності, де базисні поліноми Бернштейна виступають в ролі функцій
належності.

{\bf Зв'язок роботи з науковими програмами, планами, темами.} Робота
виконана в ННК ``Інститут прикладного системного аналізу'' НТУУ
``КПІ'' відповідно до плану науково-дослідних робіт: ``Розробка
якісно-кількісного методу ситуаційного аналізу на основі
байєсівських мереж'' (№ДР 0103U000528), ``Розробка та впровадження
автоматизованої інформаційної системи підтримки прийняття рішень в
управлінні проектами'' (№ДР 0102U000245).

{\bf Мета і завдання дослідження.} Підвищити якість прогнозування
стохастичних процесів за допомогою нових інформаційних технологій та
методів розв'язання некоректних задач відновлення залежностей. А
саме:
\begin{enumerate}

\item Створити нову інформаційну технологію синтезу нечітких
баз знань пониженої складності на основі нейронечітких моделей.

\item За допомогою технології опорних векторів забезпечити робастність
нейронечітких моделей для описання стохастичних процесів з
домінуючою випадковою складовою.

\item Вивчити недоліки існуючих методів регуляризації некоректних задач
відновлення залежностей та отримати новий удосконалений критерій
адекватності нейронечітких моделей.

\item Розробити новий метод побудови нейронечітких моделей, який забезпечить
високоякісне прогнозування стохастичних процесів. Метод повинен
враховувати апріорну інформацію про структуру моделі, працювати на
коротких вибірках і оцінювати похибки прогнозу.
% із заданим рівнем довіри.

\item На основі розроблених методів створити нову інформаційну
систему обробки статистичних даних та застосувати її до розв'язку
реальних задач прогнозування.
\end{enumerate}

{\em Об'єкт дослідження} --- стохастичні процеси.

{\em Предмет дослідження} --- нові інформаційні технології та методи
розв'язання некоректних задач відновлення стохастичних залежностей.

{\em Методика досліджень} ґрунтується на основі теорії ймовірності,
статистичного оцінювання і навчання, оптимального планування,
нечітких множин, математичного програмування, чисельних методів.
Програмну реалізацію запропонованих методів та алгоритмів виконано в
середовищах MATLAB~6.1, JDK~1.5.0.4. Для порівняння отриманих
результатів використано пакети ``Neuro-Shell~2'', %``GMDH Modeler''
``GMDH Modeler 0.9.37'' та ``NeuroSolutions~5''.
%\footnote{\url{http://www.wardsystems.com},
% \url{http://www.neurosolutions.com}}.

{\bf Наукова новизна одержаних результатів.} В дисертаційній роботі
отримані наступні нові результати:
\begin{enumerate}
\item Розвинуто метод побудови нейронечітких
моделей у формі Бернштейна. Для визначення барицентричних координат
новий прискорений метод використовує швидке обернене відображення
Кастельжо. Вперше запропоновано еволюційний метод побудови робастних
нейронечітких моделей у формі Бернштейна, де для визначення
барицентричних координат використовується оптимальне обернене
відображення Кастельжо.
%\footnote{Пряме відображення Кастельжо це
%алгоритм Поля де Кастельжо (фр. de Casteljau).}
\item Створена нейронечітка інформаційна технологія синтезу нечітких баз
знань квадратичної складності на основі так званих збалансованих
нейронечітких моделей у формі Бернштейна.

\item На основі наближення маргінальної правдоподібності методом Лапласа
отримано новий критерій адекватності байєсівської регресії опорних
векторів, який, на відміну від відомих критеріїв, не порушує
робастності моделей та має просту аналітичну форму.

\item Розроблено новий індуктивний метод побудови збалансованих
робастних нейронечітких моделей у формі Бернштейна (ПРІАМ) на основі
байєсівської регресії опорних векторів у характеристичному просторі
поліноміальних функцій Без’є-Бернштейна, який відрізняється від
аналогів підвищеною якістю прогнозування стохастичних процесів з
домінуючою випадковою складовою і можливістю задавати апріорну
структуру моделі.

\item Створено нову інформаційну систему обробки статистичних даних,
математичний апарат якої реалізує запропоновані методи. Унікальність
системи полягає в її здатності оперувати в онлайн режимі (в мережі Інтернет).%інтерактивному
\end{enumerate}

{\bf Практичне значення одержаних результатів} полягає в тому, що
запропоновані автором інформаційні технології та методи можуть бути
використані в системах підтримки прийняття рішень при математичному
моделюванні і прогнозуванні стохастичних процесів з домінуючою
випадковою складовою. Зокрема отримані результати впроваджені в
Севастопольську гідрометеорологічну обсерваторію МНС України.
%\vdots
%, а також в навчальний процес кафедри математичних методів
%системного аналізу НТУУ ``КПІ''.

\nocite{Mytnyk-Bidyuk,Bidyuk-Mytnyk,Bidyuk-Lytvinenko-Mytnyk,Mytnyk-BBR-2006,Mytnyk-KISA-07,Mytnyk-IBSS-07,
Mytnyk-5conf,Bidyuk-Baklan-Mytnyk-Lytvinenko-conf,Mytnyk-6conf,Mytnyk-ISDMIT-2005,Mytnyk-DSMSI-2005,
Mytnyk-Kravchuk-conf-2006,Mytnyk-IAI-conf-2006,Mytnyk-DSMSI-2007}

{\bf Особистий внесок здобувача.} Всі наукові результати, викладені
в дисертації, отримані автором самостійно. В написаних у
співавторстві роботах здобувачеві належать: швидке обернене
відображення Кастельжо~\cite{Mytnyk-Bidyuk}; еволюційний метод
визначення положення максимумів першої похідної спектру відбиття
листків рослинності~\cite{Bidyuk-Mytnyk,Bidyuk-Lytvinenko-Mytnyk};
новий математичний метод аналізу даних гідробіологічного
моніторингу~\cite{Mytnyk-IBSS-07}.

{\bf Апробація результатів дисертації.} Основні результати
дисертаційної роботи доповідалися і обговорювалися на міжнародних
наукових конференціях ``Системний аналіз та інформаційні
технології'' (Київ, 2003-2004 рр.), ``Стан та перспективи розвитку
новітніх науково-освітніх комп'ютерних технологій'' (Миколаїв, 2003
р.), ``Інтелектуальні системи прийняття рішень та прикладні аспекти
інформаційних технологій'' (Євпаторія, 2005 р.), ``Моделювання та
дослідження стійкості динамічних систем'' (Київ, 2005, 2007 рр.),
``Интеллектуальный анализ информации'' (Київ, 2006 р.), на
одинадцятій міжнародній науковій конференції імені академіка
М.Кравчука (Київ, 2006 р.).

{\bf Публікації.} Основні положення дисертації викладені в 14
друкованих роботах, з яких 5 у провідних фахових виданнях.

{\bf Структура та обсяг дисертації.} Дисертація складається із
вступу, чотирьох розділів, висновків, списку літератури з 212 джерел
на 24 сторінках, чотирьох додатків. Загальний обсяг роботи становить
183 сторінки, з яких 134 сторінки основного тексту.


\section*{ОСНОВНИЙ ЗМІСТ РОБОТИ}

Узагальнена задача відновлення стохастичної залежності формально
поставлена у наступному вигляді. Нехай задані дані спостережень
$\mathcal{D}=\{\left(y_j,\vect{x}_j\right):j=1,\ldots,N\}$, де
$\vect{x}=\col{x^1,\ldots,x^n}\in\mathcal{X}\subseteq\mathbb{R}^n$,
$y\in\mathcal{Y}\subseteq\mathbb{R}$. Висувається гіпотеза про
існування стохастичної залежності, відповідно до якої кожному
вектору $\vect{x}$ ставиться у відповідність число $y$, отримане за
допомогою випадкового випробування за законом $P(y|\vect{x})$.
Задача відновлення стохастичної залежності полягає у відшуканні
умовної щільності розподілу $P(y|\vect{x})$ на основі скінченої
вибірки даних $\mathcal{D}$. Дана обернена задача некоректно
поставлена, оскільки має більш ніж один розв'язок, і зводиться до
пошуку щільності апостеріорного прогнозного розподілу
$P(y|\vect{x},\mathcal{D})$, який є наближенням реальної
стохастичної залежності. Наскільки близьким буде таке наближення
залежить від повноти даних спостережень. Процес побудови
$P(y|\vect{x},\mathcal{D})$, або його умовного математичного
сподівання, тобто функції регресії $y(\vect{x})=\mean{y_x}=\int
yP(y|\vect{x},\mathcal{D})dy$, будемо називати емпіричним
моделюванням.

В {\bf першому розділі} розглянуті основні проблеми емпіричного
моделювання, серед яких некоректність постановки самої задачі
навчання за даними спостережень і слабка структурованість
представлення моделей. Значна увага приділяється механізму
регуляризації задачі навчання як засобу для розкриття невизначеності
відношення зміщення-варіації моделі. Розглянутий розклад аналізу
варіацій як засіб забезпечення структурованості представлення
моделі. Наведені типові приклади використання цього розкладу. Перший
розділ закінчується постановкою задач дисертаційного дослідження.

{\bf Другий розділ} присвячений дослідженню нейронечітких
технологій. Введене формальне означення нейронечітких моделей
відповідно до якого нейронечітка модель в канонічній формі має
вигляд:
$$
f(\vect{x})=\sum_{i}w_i\mu_{A^i}(\vect{x}),\quad\sum_{i}\mu_{A^i}(\vect{x})=1,
$$
де $\mu_{A^i}$ --- дійсні невід'ємні функції належності змінної
$\vect{x}$, $A^i$ --- лінгвістичні терми визначені на $\mathcal{X}$.
Як відомо, основним результатом для нейронечітких моделей є наступне
твердження. Дефазифікований методом середнього значення результат
нечіткого логічного висновку системи Мамдані, в якій функції
нечіткої логіки визначені алгебраїчними операторами добутку та суми,
відповідає виходу нейронечіткої моделі. При цьому, якщо для
представлення входу та виходу нечіткої системи Мамдані
використовуються відповідно дійсні невід'ємні функції належності
$\mu_{A^i}(\vect{x})$ та $\mu_{B^j}(y)$, де $A^i$, $B^j$
--- лінгвістичні терми визначені відповідно на $\mathcal{X}$ та
$\mathcal{Y}$, і $\sum_{i}\mu_{A^i}(\vect{x})=1$, а також $\forall
j:\int\mu_{B^j}(y)dy=const$, тоді
\begin{equation}\label{Confidence-systems}
 w_i=\sum_{j}c_{ij}y_j^c,\quad 1=\sum_{j}c_{ij},\quad
0\leq c_{ij}, \quad
y_j^c=\frac{\int\mu_{B^j}(y)ydy}{\int\mu_{B^j}(y)dy},
\end{equation}
де $c_{ij}$ --- вага правила: {\em якщо} $\vect{x}\in A^i$ {\em
тоді} $y\in B^j$. Тож задача генерації нечітких правил на основі
аналітичної нейронечіткої моделі зводиться до відшукання
ймовірностей $c_{ij}$ за заданими коефіцієнтами $w_i$. Для цього
необхідно розв'язати системи лінійних рівнянь з
обмеженнями~(\ref{Confidence-systems}).

В загальному випадку функції належності визначають не для всього
вектору входу $\vect{x}$, а для кожної вхідної змінної $x^k$ окремо.
Показано, що в цьому випадку нечітка база знань має експонентну
складність відносно розмірності вектору входу. З метою зменшення
кількості нечітких правил і спрощення самих правил досліджені
нейронечіткі моделі у формі Бернштейна, які гарантують поліноміальну
складність нечіткої бази знань. Для представлення регресії
$y(\vect{x})$ модель у формі Бернштейна має вигляд:
\begin{equation}\label{gabor-expan}
f(\vect{x}) =
b+\sum_{k=1}^nB^d_k(x^k)+\sum_{p=1}^{n-1}\sum_{q=p+1}^nB^d_{pq}\left(x^p,
x^q\right),
\end{equation}
де поліноми у формі Бернштейна $B^d_k$, $B^d_{pq}$ однієї та двох
змінних відповідно визначаються як лінійні комбінації базисних
поліномів Бернштейна порядку $d$ від барицентричних координат $s$ та
$\vect{u}=\col{u, v}$:
%\begin{equation}
%\label{BB-polynomial-functions} %
$$
B^d_k(x^k)=\sum_{j=0}^dw^k_j\phi_j^d\bigl(s(x^k)\bigr),\quad
B^d_{pq}(x^p,x^q)=\sum_{i+r+t=d}w^{pq}_{irt}\phi_{irt}^d\bigl(\vect{u}(x^p,x^q)\bigr).
$$
%\end{equation}
Базисні поліноми Бернштейна однієї та двох змінних мають вигляд:
$$
\phi_j^d(s)=\dbinom{d}{j}s^j(1-s)^{d-j},\quad
\phi_{irt}^d(\vect{u})=\dbinom{d}{i,r,t} u^iv^r(1-u-v)^t.
$$
Барицентричні координати $s$ та $\vect{u}$ обчислюються за допомогою
оберненого відображення Кастельжо $\Psi_k(\Lambda_k) : x^k\mapsto
s(x^k)\in [0; 1]$, та $\Psi_{pq}(\Lambda_{pq}) : (x^p, x^q)\mapsto
\vect{u}(x^p,x^q)\in \triangle\{u\geqslant0, v\geqslant0,
u+v\leqslant1 \}$ на основі систем контрольних точок
$\Lambda_{k}=\{a_j\in {\mathbb R}:j=0,\ldots,d\}$,
$\Lambda_{pq}=\{\vect{a}_{ijk}\in {\mathbb R^2}:i+j+k=d\}$. Обернене
відображення Кастельжо використовує чисельний метод зворотного
розповсюдження помилки для розв'язання рівнянь:
$$
x^k=\sum_{j=0}^da_j\phi_j^d(s),\quad\col{x^p,x^q}=\sum_{i+j+k=d}\vect{a}_{ijk}\phi_{ijk}^d(\vect{u}).
$$
Нейронечітку модель у формі Бернштейна можна представити як лінійну
в параметрах модель
$\mathcal{M}(\vect{x},\vect{w})=b+\vect{w}\cdot\vect{\Phi}(\vect{x})$
в евклідовому просторі $\mathcal{W}$ із скалярним добутком
$\vect{x}\cdot\vect{z}=\sum_ix_iz_i$. Простір $\mathcal{W}$ будемо
називати простором поліноміальних функцій
Без'є-Бернштейна.
%\footnote{Поліноми у формі Бернштейна також
%називають поліноміальними функціями Без'є-Бернштейна.}.
Вектор параметрів
$\vect{w}=\col{\ldots,w^k_j,\ldots,w^{pq}_{irt},\ldots}\in\mathcal{W}$.
Вектор-функція $\vect{\Phi}:\mathcal{X}\rightarrow\mathcal{W}$
складається з функцій належності однієї $\{\phi^d_i(\cdot)\}$ та
двох $\{\phi^d_{irt}(\cdot,\cdot)\}$ змінних
$\vect{\Phi}:\vect{x}\mapsto\col{\ldots,\phi^d_j(x^k),\ldots,\phi^d_{irt}(x^p,x^q),\ldots}$.

Для нейронечітких моделей у формі Бернштейна нечіткі правила
генеруються незалежно на основі окремих нейронечітких підмоделей у
канонічній формі. Кожна нейронечітка підмодель визначається
поліномом у формі Бернштейна. При цьому функції належності це
базисні поліноми Бернштейна від барицентричних координат.

В результаті дослідження нейронечітких моделей у формі Бернштейна
виявлені такі недоліки. Повільна збіжність оберненого відображення
Кастельжо на основі чисельного методу зворотного розповсюдження
помилки. Конфігурація контрольних точок не визначена однозначно.
Необґрунтована можливість незалежного використання нечітких правил,
які сформовані на основі окремих підмоделей. Функції належності у
формі базисних поліномів Бернштейна високих порядків сильно
перекриваються.

В роботі розроблено прискорений метод навчання нейронечітких моделей
у формі Бернштейна. Основою методу є швидке обернене відображення
Кастельжо, зміст якого полягає в наступному. Нехай для довільної
множини точок $\{\vect{z}\}\subset {\mathbb R^2}$ задана система
контрольних точок $\Lambda=\{\vect{a}_{ijk}\in {\mathbb
R^2}:i+j+k=d\}$ порядку $d$ така, що задовольняє умовам:
\begin{enumerate}
\item Опукла оболонка $\hull{\vect{a}_{ijk}}\supset\{\vect{z}\}$.
\item Множина внутрішніх точок $\inter\hull{\vect{a}_{ijk}}\neq\emptyset$.
\item Виконуються наступні рівності
$\quad2\vect{a}_{i+1j+1k}=\vect{a}_{ij+2k}+\vect{a}_{i+2jk}$,\\
$2\vect{a}_{ij+1k+1}=\vect{a}_{ij+2k}+\vect{a}_{ijk+2}$,
$2\vect{a}_{i+1jk+1}=\vect{a}_{i+2jk}+\vect{a}_{ijk+2}$,
$i+j+k+2=d$.
\end{enumerate}
Тоді $\Psi(\Lambda):\vect{z}\mapsto\vect{u}(\vect{z}) = \left(
\vect{a}_{0d0} - \vect{a}_{00d}, \vect{a}_{d00} -
\vect{a}_{00d}\right)^{-1}\left(\vect{z} - \vect{a}_{00d} \right)$,
і при цьому $\vect{u}\in \triangle\{u\geqslant0, v\geqslant0,
u+v\leqslant1 \}$. Аналогічний результат отримано також для
оберненого відображення Кастельжо однієї змінної. Таким чином
виведена чітка схема розміщення контрольних точок, яка забезпечує
більш швидке (в 3--4 рази) навчання нейронечітких моделей у формі
Бернштейна.

Досліджено також питання про оптимальне, за заданим критерієм,
розміщення контрольних точок. Відповідно введено ряд означень
оптимальних систем контрольних точок. Зокрема A-оптимальною системою
контрольних точок будемо називати таку, що мінімізує слід
коваріаційної матриці оцінок коефіцієнтів $\vect{w}$:
$\Lambda_{\mbox{\small{A-опт}}}=\arg\min\tr\cov\left(\hat{\vect{w}}\left(\Lambda\right)\right)$.
Як відомо, А-оптимальність мінімізує середню дисперсію оцінок
коефіцієнтів, що має зміст суми квадратів головних півосей еліпсоїду
розсіювання оцінок. Оскільки А-оптимальність покращує обумовленість
інформаційної матриці, то підвищується робастність побудованої
моделі. Для розв'язання такої оптимізаційної задачі запропоновано
використовувати еволюційні методи, зокрема генетичний алгоритм.

Для обґрунтування незалежного використання нейронечітких підмоделей
вводяться так звані збалансовані нейронечіткі моделі у вигляді:
$$
f(\mathbf{x})=b+
\sum_{k=1}^ng_k(x^k)+\sum_{p=1}^{n-1}\sum_{q=p+1}^ng_{pq}\left(x^p,
x^q\right),
$$
$$
g_k(x^k)=\sum_iw_i^k\mu_{A^i_k}(x^k),\quad
g_{pq}(x^p,x^q)=\sum_jw_j^{pq}\mu_{A^j_{pq}}(x^p,x^q),
$$
де математичне сподівання функцій $g_k$, $g_{pq}$ за умови
багатовимірного рівномірного розподілу випадкової величини
$\mathbf{x}$ на своїй області визначення дорівнює нулю. При цьому
нейронечіткі підмоделі визначаються як $f_k=b+g_k$ та
$f_{pq}=b+g_{pq}$. Величина $b$ називається зміщенням нейронечіткої
збалансованої моделі і характеризує середнє значення виходу. Таким
чином, за відсутності знань щодо значення змінної $x^k$ логічно
припустити (за принципом максимальної ентропії), що змінна $x^k$ з
однаковою ймовірністю може приймати будь-яке значення зі своєї
області визначення. При цьому вплив цієї змінної на вихід моделі
після розкриття невизначеності за допомогою математичного сподівання
буде нульовим. Тож збалансованість дозволяє генерувати нечіткі
правила на основі окремої підмоделі незалежно від інших підмоделей.
Визначена достатня умова збалансованості нейронечіткої моделі:
$$
\forall i: \meanl{\mu_{A^i_k}(x^k)}=const_k,\quad\forall j:
\meanl{\mu_{A^j_{pq}}(x^p,x^q)}=const_{pq},\quad\mathbf{x}\sim
\mathcal{U}(\mathcal{X}),
$$
а також сума коефіцієнтів при функціях належності рівна нулю:
$\sum_iw_i^k=0$, $\sum_jw_j^{pq}=0$. Показано, що нейронечіткі
моделі у формі Бернштейна можна звести до збалансованих і отримати
збалансовані нейронечіткі моделі у формі Бернштейна. При цьому
обернене відображення Кастельжо повинно бути побудоване так, щоб
рівномірний розподіл змінної $\mathbf{x}$ індукував рівномірний
розподіл відповідних барицентричних координат. Швидке обернене
відображення Кастельжо задовольняє цій умові.


В {\bf третьому розділі} викладені основи РОВ. Розглянута
байєсівська РОВ, яка будується наступним чином. Нехай систематична
складова стохастичної залежності може бути представлена деякою
параметричною моделлю $\mathcal{M}$ із простору моделей
$\mathcal{H}$. Нехай задано простір моделей $\mathcal{H}$. Тоді,
відповідно до байєсівської теорії прийняття рішень, в якості
оптимальної вибирається модель з найбільшою апостеріорною
ймовірністю $P(\mathcal{M}|\mathcal{D})$. Припускаючи рівномірний
розподіл моделей в просторі $\mathcal{H}$, моделі ранжуються за
величиною їх маргінальної правдоподібності
$P(\mathcal{D}|\mathcal{M})$ (підтвердження), яка виражає здатність
моделі $\mathcal{M}$ генерувати дані $\mathcal{D}$. Маргінальна
правдоподібність визначається як:
\begin{equation}\label{int_evi}
P(\mathcal{D}|\mathcal{M})=\int_\mathcal{W}P(\mathcal{D}|\vect{w},\mathcal{M})F(d\vect{w}|\mathcal{M}),
\end{equation}
де $F$ --- апріорна ймовірнісна міра на $\mathcal{W}$. Ймовірність
$P(\mathcal{D}|\vect{w},\mathcal{M})$ виражає здатність моделі
$\mathcal{M}$ з заданими параметрами $\vect{w}$ генерувати дані
$\mathcal{D}$ і називається правдоподібністю. Оскільки
підтвердження~(\ref{int_evi}) може бути обчислена аналітично тільки
в найпростіших випадках, наприклад у випадку нормального шуму, то
застосовують різноманітні методи наближення. Серед них метод
Монте-Карло, розповсюдження сподівань, варіаційні методи, метод
Лапласа.

Для того щоб отримати точкову оцінку параметрів моделі, байєсівська
теорія прийняття рішень мінімізує умовний байєсівський ризик:
$$
R_\mathcal{L}(\hat{\vect{w}})=\meanl{\mathcal{L}(\hat{\vect{w}},\vect{w})}=\int_\mathcal{W}
\mathcal{L}(\hat{\vect{w}},\vect{w})F(d\vect{w}|\mathcal{D},\mathcal{M}),\quad
\mathcal{L}(\hat{\vect{w}},\vect{w})=\lim_{t\rightarrow0}|\hat{\vect{w}}-\vect{w}|^t,
$$
де $\mathcal{L}$ --- такий функціонал ризику, при якому
%прийняти оцінку $\hat{\vect{w}}$.
оптимальна байєсівська оцінка параметрів моделі це мода
апостеріорного розподілу параметрів (МАП оцінка):
$$
\vect{w}_{\mbox{мап}}=\arg\min\limits_{\vect{w}\in\mathcal{W}}R_\mathcal{L}(\vect{w})
=\arg\max\limits_{\vect{w}\in\mathcal{W}}P(\vect{w}|\mathcal{D},\mathcal{M}).
$$

Байєсівська РОВ будується за наступних припущень. Випадкова складова
стохастичної залежності вважається адитивним шумом і
$y_j=\mathcal{M}(\vect{x}_j,\vect{w})+\delta_j$, де $\delta_j$ ---
незалежні і однаково розподілені за законом
$P(\delta|\vect{w},\mathcal{M})$ випадкові величини. Апріорна
щільність розподілу параметру $\vect{w}$ моделі $\mathcal{M}$ згідно
з принципом максимальної ентропії
%\footnote{Згідно з принципом
%максимальної ентропії знаходять розподіл, який з максимізує
%інформаційну ентропію процесу і при цьому задовольняє наявній
%апріорній інформації про процес. В класі розподілів із заданими
%першими двома моментами таким оптимальним апріорним розподілом є
%нормальний розподіл.}
задана у вигляді багатовимірного нормального
розподілу з нульовим математичним сподіванням і одиничною матрицею
коваріації:
$P(\vect{w}|\mathcal{M})=\mathcal{N}(\vect{w}|\vect{0},\mathbf{I})$.
Моделі розглядаються як лінійні в параметрах в деякому
характеристичному просторі $\mathcal{W}$:
$\mathcal{M}(\vect{w},\vect{x})=b+
\vect{w}\cdot\vect{\Phi}(\vect{x})$,
$\vect{\Phi}:\mathcal{X}\rightarrow\mathcal{W}$. Гомоскедастична
модель шуму задана у вигляді:
\begin{equation}\label{noise_mod}
P(\delta_j|\vect{w},\mathcal{M})=\frac{\beta}{2(1+\epsilon\beta)}\exp(-\beta|\delta_j|_\epsilon),
\end{equation}
де $|\cdot|_\epsilon$ це $\epsilon$-нечутлива функція втрат Вапника.
$|\delta|_\epsilon=0$ для $|\delta|\leqslant\epsilon$,
$|\delta|_\epsilon=|\delta|-\epsilon$ для $|\delta|>\epsilon$. Така
функція втрат забезпечує розрідженість представлення і робастність
моделей РОВ. Параметри $\epsilon$ і $\beta$ називають
гіперпараметрами.

Для заданої моделі $\mathcal{M}$ МАП оцінка байєсівської РОВ
породжує задачу мінімізації регуляризованого ризику:
\begin{equation*}
R_{\mbox{рег}}(\vect{w})=\beta\sum_{j=1}^N|\delta_j|_\epsilon+\frac{1}{2}\|\vect{w}\|^2\longrightarrow\min_\vect{w},
\end{equation*}
яка зводиться до типової задачі опуклого квадратичного програмування
з розв'язком у вигляді:
$$
\begin{array}{cc}
f_{\mbox{мап}}(\vect{x})=\sum_j(\alpha_j-\alpha_j^\ast)\vect{\Phi}(\vect{x}_j)\cdot\vect{\Phi}(\vect{x})+b_{\mbox{мап}},~
\vect{w}_{\mbox{мап}}=\sum_j(\alpha_j-\alpha_j^\ast)\vect{\Phi}(\vect{x}_j),\\
b_{\mbox{мап}}=\mbox{середнє}_j\left\{\begin{array}{c}y_j-\vect{w}\cdot\vect{\Phi}(\vect{x}_j)-\epsilon,~\alpha_j\in(0,\beta)\\
y_j-\vect{w}\cdot\vect{\Phi}(\vect{x}_j)+\epsilon,~\alpha_j^\ast\in(0,\beta)\end{array}\right\},\\
\end{array}
$$
де $\alpha_j$, $\alpha_j^\ast$ --- множники Лагранжа, які
задовольняють $\sum_{j=1}^N\left(\alpha_j-\alpha_j^\ast\right)=0$,
$\alpha_j,\alpha_j^\ast\in[0;\beta]$. Таким чином параметри і сама
модель представлені як лінійна комбінація елементів розрідженої
множини навчальної вибірки в характеристичному просторі, для яких
один з множників $\alpha_j$ або $\alpha_j^\ast$ ненульовий. Такі
елементи називають опорними векторами. Опорні вектори лежать поза
$\epsilon$-смугою нечутливості або на її верхній чи нижній межах.

Більш складною проблемою є обчислення підтвердження моделі.
Асимптотично точну оцінку можна отримати за допомогою методу
Монте-Карло. Відносно точну оцінку дає метод розповсюдження
сподівань. В роботі використано метод Лапласа, який хоч і є неточним
в порівнянні з вище згаданими методами, проте потребує значно менші
обчислювальні витрати. Аналіз показує, що цей метод систематично
переоцінює підтвердження і відтак не може бути використаний для його
точного підрахунку. Але, оскільки метод Лапласа переоцінює
підтвердження систематично для всіх моделей, він може бути
використаний для швидкого порівняння адекватності моделей. Метод
Лапласа дає оцінку:
\begin{equation}\label{Log-Evidence-SVR-Estimate-intro}
-\ln P(D|\mathcal{M})\approx
R_{\mbox{рег}}(\vect{w}_{\mbox{мап}})-N\ln\frac{\beta}{2(1+\epsilon\beta)}
+\frac{1}{2}\ln\det\mathbf{H}_{\mbox{мап}},
\end{equation}
де
$\mathbf{H}_{\mbox{мап}}=\nabla^2_\vect{w}R_{\mbox{рег}}(\vect{w}_{\mbox{мап}})$
--- гесcіан регуляризованого ризику в точці, яка є модою
апостеріорного розподілу. При цьому основною проблемою є
недиференційовність функції втрат в критичних точках, які лежать на
верхній та нижній межах $\epsilon$-смуги. Тому, як правило,
застосовують різноманітні згладжені аналоги. Але вони порушують
розрідженість та робастність РОВ і призводять до складного
байєсівського висновку. В роботі запропоновано використовувати
локально згладжену в нескінченно малому околі критичних точок
функцію втрат і доведено, що для будь-якої $\epsilon$-нечутливої
смуги РОВ існує як завгодно мале відхилення $\Delta\epsilon$ таке,
що для $\epsilon_1$-нечутливої смуги
($\epsilon_1=\epsilon-\Delta\epsilon$) гессіан регуляризованого
ризику $\mathbf{H}_{\mbox{\rm мап}}=\mathbf{I}$ і при цьому вектори
зберігають властивість опорності. Як наслідок, отримано новий
критерій байєсівського підтвердження (КБП) адекватності моделей РОВ:
\begin{equation}\label{BECdef}
-\ln
P(\mathcal{D}|\mathcal{M})\approx{\mbox{КБП}}(\mathcal{M},\epsilon,\beta)=
R_{\mbox{рег}}(\vect{w}_{\mbox{мап}})-N\ln\frac{\beta}{2(1+\epsilon\beta)}.
\end{equation}
Задача максимізації підтвердження зводиться до задачі мінімізації
КБП. Оскільки КБП залежить від гіперпараметрів, то в межах тієї
самої моделі можна знайти оптимальні значення гіперпараметрів, які
мінімізують~(\ref{BECdef}). Цю задачу розв'язано рефлексивним
методом Ньютона, де на кожному кроці будується РОВ і знаходиться
$\vect{w}_{\mbox{мап}}$.

Основні переваги байєсівської РОВ полягають в наступному. Розклад
опорних векторів не залежить від розмірності вхідного простору.
Задача квадратичного програмування дає єдиний розв'язок. Модель РОВ
робастна. Байєсівський підхід дозволяє робити оцінку для довірчих
інтервалів. КБП забезпечує високу швидкість перебору моделей. Серед
недоліків слід відзначити неточне обчислення підтвердження. Далі в
роботі досліджені питання вибору простору моделей і
характеристичного простору. З метою поєднання переваг байєсівської
РОВ із зручністю нейронечіткого підходу Харріса в якості
характеристичного простору використано простір поліноміальних
функцій Без'є-Бернштейна, де для обчислення барицентричних координат
використовується швидке обернене відображення Кастельжо.

Введено поняття конфігурації характеристичного простору
поліноміальних функцій Без'є-Бернштейна у вигляді верхньотрикутної
матриці $\mathbf{C}$ розмірності $n\times n$, де кожен елемент
$c_{i\geqslant j}\geqslant0$ відображає ступінь впливу фактору
$x_{i=j}$ або пари факторів $x_i$, $x_j$ на вихідну змінну $y$ і
визначає порядок базисних поліномів Бернштейна для відповідної
поліноміальної функції. Характеристичне відображення у цьому випадку
матиме вигляд:
$\vect{\Phi}(\mathbf{C}):\vect{x}\mapsto\col{\ldots,\phi_j^{c_k}(x^k),\ldots,\phi_{irt}^{c_{pq}}(x^p,
x^q),\ldots}$. Це відображення індукує простір моделей у формі
Бернштейна
$\mathcal{H}=\spanv{\ldots,\phi_j^{c_k},\ldots,\phi_{irt}^{c_{pq}},\ldots}$,
які узагальнюють~(\ref{gabor-expan}):
\begin{equation}\label{nfms}
\mathcal{H}\ni\mathcal{M}(\vect{w},\vect{x})
=b+\sum_{k=1}^nB^{c_k}_k(x^k)+\sum_{p=1}^{n-1}\sum_{q=p+1}^nB^{c_{pq}}_{pq}\left(x^p,
x^q\right),
\end{equation}
Для загальності міркувань покладемо $B^0(\cdot)\equiv0$, що фактично
означає відсутність впливу відповідного фактору або пари факторів на
вихід. Доведено, що РОВ в характеристичному просторі поліноміальних
функцій Без'є-Бернштейна індукує збалансовані нейронечіткі моделі у
формі Бернштейна, які можна використовувати для побудови нечітких
правил на основі окремих підмоделей: $f_k(x^k)=b+B^{c_k}_k(x^k)$,
$f_{pq}\left(x^p, x^q\right)=b+B^{c_{pq}}_{pq}\left(x^p,
x^q\right)$.

Основним результатом третього розділу є індуктивний метод побудови
збалансованих нейронечітких моделей у формі Бернштейна, який
визначає порядок перебору моделей в просторі $\mathcal{H}$. Схема
алгоритму ПРІАМ (Поліноміальної РОВ Індуктивний Алгоритм
Моделювання), який реалізує даний метод, має вигляд:

\begin{algorithmic}
   \STATE {\bfseries Задані:} дані спостережень $\mathcal{D}$, рівень збіжності $\mu>0$,
 початкова модель $\mathcal{M}^{(0)}=\mathcal{H}\left(\mathbf{C}^{(0)}\right)$, яка відповідає апріорним
 сподіванням.
   \STATE {\bfseries Результат:} субоптимальна модель
   $\mathcal{M}_{\mbox{opt}}$.
   \STATE {\bfseries Алгоритм починає роботу з}
   \STATE нормування вхідного та вихідного просторів; ітератор $t\longleftarrow$0;%
   \REPEAT
   \STATE $\mathcal{M}_{\mbox{opt}}\longleftarrow\mathcal{M}^{(t)}$;%
   \STATE генерація множини моделей кандидатів:
   $\left\{\mathcal{M}_{ij}^{(t+1)}=\mathcal{H}\left(\mathbf{C}^{(t+1)}_{ij}\right)\right\}_{ij}$,\\%
   де $\mathbf{C}^{(t+1)}_{ij}=\mathbf{C}^{(t)}\pm\mathbf{1}_{ij}$,
    $1\leqslant i\leqslant j\leqslant n$, $\mathbf{1}_{ij}$ --- нульова матриця з одиницею в
рядку $i$ в стовпчику $j$, що відповідає мінімальній зміні конфігурації;\\
   \FOR{{\bfseries кожної} моделі кандидата $\mathcal{M}_{ij}^{(t+1)}$}
   \STATE обчислення $\mbox{КБП}\left(\mathcal{M}_{ij}^{(t+1)}\right)$, мінімізація~(\ref{BECdef})
          за гіперпараметрами;%
   \ENDFOR перебору моделей кандидатів;
   \STATE вибір моделі з найменшим значенням КБП:
   \STATE $\mathcal{M}^{(t+1)}=\arg\left(\mbox{КБП}^{(t+1)}=\min\mbox{КБП}\left(\mathcal{M}_{ij}^{(t+1)}\right)\right)$;%
   \STATE $t\longleftarrow t+1$;
   \UNTIL{не виконається критерій зупинки: $\mbox{КБП}^{(t)}+\mu>\mbox{КБП}^{(t-1)}$};%
   \STATE {\bfseries кінець роботи алгоритму.}
\end{algorithmic}

Якщо визначити розмірність задачі навчання розмірністю вхідного
простору $n$ і об'ємом вибірки даних спостережень $N$, то складність
алгоритму ПРІАМ складається з витрат на перебір моделей кандидатів і
витрат на розв'язок задачі квадратичного програмування методом
активних обмежень і становить $\mathcal{O}(n^2N^3)$. Збіжність ПРІАМ
забезпечується глобальною і квадратичною збіжністю рефлексивного
методу Ньютона для мінімізації КБП за гіперпараметрами а також, як
легко показати в силу оцінки знизу
$\mbox{КБП}>-N\ln(\beta_{\max}/2)$ для $\beta_{\max}<\infty$,
лінійною збіжністю пошуку субоптимальної моделі.

Перша частина {\bf четвертого розділу} присвячена розробці
архітектури нової інформаційної онлайн системи обробки статистичних
даних, яка використовує розроблені в роботі нові алгоритми та
методи. Так, інформаційна система представлена на трьох рівнях:
концептуальному, логічному та фізичному. На концептуальному рівні
визначені інформаційні процеси, процедури та потоки
(рис.~\ref{fig:inf_system}а). Відповідно користувач може як
поповнювати базу вибірок новими вибірками так і завантажувати та
обробляти вибірки алгоритмами з бази алгоритмів. Хостинг провайдер
підтримує сайт та сервери, на яких розміщуються бази вибірок та
алгоритмів. На логічному рівні описана взаємодія формалізованих
моделей інформаційних процесів. На фізичному рівні створені
підсистеми, які реалізують процеси обробки, накопичення даних та
представлення знань. Схема носіїв цих підсистем зображена на
рис.~\ref{fig:inf_system}б.
\begin{figure*}
\hbox{%
\makebox[7mm]{}\makebox[150mm][c]{\includegraphics[width=150mm]{sd_dfd.pdf}}%
}%
\hbox{ \makebox[165mm][c]{(a)}%
}%
\hbox{%
\makebox[7mm]{}\makebox[150mm][c]{\includegraphics[width=150mm]{sd_depd.pdf}}%
}%
\hbox{ \makebox[165mm][c]{(б)}%
}%
\caption{(a) Діаграма інформаційних потоків. (б) Діаграма топології
інформаційної системи.} \label{fig:inf_system}
\end{figure*}
Користувач зі свого терміналу з браузером, який підтримує JRE версії
не нижче 1.4, має можливість через Інтернет завантажити java-аплети,
вибірки даних та будувати прогнозуючі моделі. Аплети взаємодіють з
базою вибірок через JDBC. Java-аплети представляють собою написані
на мові Java алгоритми: нейронечіткий алгоритм Харріса --- B-B.NF та
ПРІАМ --- PRIAMUS. Структура аплетів та послідовність взаємодії
користувача з аплетами зображена на рис.~\ref{fig:inf_system_seq}.
Оскільки реалізовані механізми багатопоточності, то відповідно
користувач має можливість отримати список задач (вибірок) та
алгоритмів, вибрати необхідний алгоритм та задачу і завантажити
паралельну сесію. В цій сесії користувач налаштовує параметри задачі
та алгоритму, будує модель та отримує детальні результати прогнозу
на основі цієї моделі.

\begin{figure*}
\hbox{%
\makebox[7mm]{}\makebox[150mm][c]{\includegraphics[width=150mm]{sd_seqd.pdf}}%
}%
\caption{Діаграма взаємодії користувача з аплетами.}
\label{fig:inf_system_seq}
\end{figure*}

В другій частині четвертого розділу наведені результати практичного
застосування реально створеного прототипу такої інформаційної
системи для відомих штучних та реальних еталонних моделей, для
спрощеної моделі макроекономіки України (ІСЦ, РСГ), для
гідрометеорологічної моделі вітрових хвиль (СГМО), для
гідробіологічної моделі біомаси макрозообентосу (ІБПМ). Для кожної з
моделей визначена оптимальна конфігурація характеристичного
простору, побудований прогноз і довірчі інтервали, виконано
порівняльний аналіз з такими провідними в області прогнозування
алгоритмами як метод групового врахування аргументів (МГВА),
нечіткий МГВА (НМГВА), рекурентні нейронні мережі (РНМ) та ANFIS.
Також наведені представлення у вигляді розкладу опорних векторів,
ітерації байєсівського виводу оптимальних значень гіперпараметрів,
залишки навчальної та прогнозної вибірок, залежності КБП та середньо
квадратичної помилки в нормованому просторі (СКПн) від ширини смуги
нечутливості. Наведений в табл.~\ref{tbl-results} порівняльний
аналіз підтверджує високу ефективність розробленого автором
індуктивного методу.
\begin{table}
\tablecaption{Порівняння СКПн прогнозу для різних методів}
\label{tbl-results} %\vskip 0.15in
\begin{center}
\begin{small}
\begin{tabular}{lcccccccc}
  \hline
  Моделі            & Longley & Filippelli & Friedman & AMPG    & ІСЦ   & РСГ    & СГМО     & ІБПМ\\
  \hline
  $N_{\mbox{повна}}(N)$& 16(11)  & 82(33)     & 1000(500)& 392(300)& 24(14)& 24(14) & 166(100)  & 50(25)\\
  $n$               & 6       & 1          & 10       & 4       & 4     & 2      & 3        & 2\\
%  $F_{\genfrac{}{}{0cm}{2}{N-n,}{N-n}}^{(80\%)}$
  $F_{N-n,N-n}^{(80\%)}$
                    & 2.2     & 1.3        & 1.0      & 1.0     & 1.8   & 1.7    & 1.1      & 1.4\\
  \hline
  ПРІАМ             & 0.017   & 0.004      & 0.009    & 0.022   & 0.003 & 0.027  & 0.033    & 0.076\\
  МГВА              & 0.051   & 0.016      & 0.029    & 0.024   & 0.130 & 0.042  & 0.025    & 0.124\\
  НМГВА             & 0.001   & 0.039      & 0.037    & 0.026   & 0.008 & 0.098  & 0.043    & 0.053\\
  РНМ               & 0.018   & 0.012      & 0.008    & 0.028   & 0.090 & 0.101  & 0.033    & 0.083\\
  ANFIS             & 0.002   & 0.001      & 0.009    & 0.027   & 0.003 & 0.051  & 0.031    & 0.110\\
  \hline
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
Для перевірки значимості переваги одного методу над іншим необхідно
порівняти відношення СКПн цих методів з наведеним табличним
значенням критерію Фішера. Так, було визначено, що в середньому
точність прогнозу ПРІАМ вища за точність МГВА на 52\%, РНМ --- на
40\%, НМГВА --- на 26\%, ANFIS --- на 10\%.

Прототип інформаційної системи впроваджено в Севастопольську
гідрометеорологічну обсерваторію для прогнозування висоти хвиль в
Севастопольській бухті. Для побудови моделі використані дані
щоденних спостережень за параметрами вітрових хвиль (висота, період,
довжина, напрям) та показниками вітру (напрям, швидкість, тривалість
дії) з 1960 по 2006 рік. Наприклад, за допомогою ПРІАМ на основі
навчальної вибірки в 100 точок (за 2006 рік) отримана оптимальна
модель у формі Бернштейна в нормованому просторі:
\begin{multline*}
f(\mathbf{x})=0.47+0.09\phi_0^1(x^1)-0.09\phi_1^1(x^1)-\\
-0.04\phi_0^1(x^2)+0.04\phi_1^1(x^2)
-0.35\phi_0^1(x^3)+0.35\phi_1^1(x^3),
\end{multline*}
де $x^1$ ---
синус кута напряму вітру від напряму на північ за годинниковою
стрілкою, $x^2$ --- косинус кута напряму, $x^3$ --- швидкість вітру.
Для генерації нечітких правил випишемо нейронечіткі підмоделі в
канонічній формі:
$$
\begin{array}{c}
f_1(x^1)=0.56\phi_0^1(x^1)+0.38\phi_1^1(x^1),\quad
f_2(x^2)=0.43\phi_0^1(x^2)+0.51\phi_1^1(x^2),\\
f_3(x^3)=0.12\phi_0^1(x^3)+0.82\phi_1^1(x^3).
\end{array}
$$
Функції належності для змінних входу визначаються через базисні
поліноми Бернштейна першого порядку $\phi_0^1$, $\phi_1^1$. Вони
відповідають нечітким змінним: західний та східний, південний та
північний, слабкий та сильний вітер. Визначимо функції належності
для висоти хвилі через п'ять B-сплайнів першого порядку побудовані
на вузлах $\{-0.25;~0;~0.25;~0.5;~0.75;~1;~1.25\}$, які відповідають
нечітким змінним: штиль, слабке хвилювання, помірне хвилювання,
сильне хвилювання, шторм з центрами в точках
$\{0;~0.25;~0.5;~0.75;~1\}$. Ці точки описують висоту хвилі від 0.1
до 2.4 метра. Розв'язавши по дві системи~(\ref{Confidence-systems})
для кожної з трьох підмоделей отримаємо нечітку базу знань з шести
простих правил:
$$
\left[
\begin{array}{l}
\mbox{\small\sc Правило 1: якщо вітер західний тоді хвилювання помірне (0.76) або сильне (0.24)}\\
\mbox{\small\sc Правило 2: якщо вітер східний тоді хвилювання слабке (0.48) або помірне (0.52)}\\
\mbox{\small\sc Правило 3: якщо вітер південний тоді хвилювання слабке (0.28) або помірне (0.72)}\\
\mbox{\small\sc Правило 4: якщо вітер північний тоді хвилювання помірне (0.96) або сильне (0.04)}\\
\mbox{\small\sc Правило 5: якщо вітер слабкий тоді штиль (0.52) або слабке хвилювання (0.48)}\\
\mbox{\small\sc Правило 6: якщо вітер сильний тоді сильне хвилювання (0.72) або шторм (0.28)}\\
\end{array}\right.
$$
На відміну від моделей, які згенеровані за допомогою МГВА, РНМ чи
навіть ANFIS, цей набір правил є абсолютно прозорим для експертів.
Експерт легко може визначити ступінь впливу того чи іншого фактору.
Можна також визначити адекватність моделі реальним представленням
експерта про процес. Так, наприклад, очевидно, що північний та
західний вітер створюють більші хвилі, ніж південний та східний. Це
збігається з уявленнями експертів про процеси в бухті, оскільки
вихід у відкрите море з бухти має північно-західний напрям.

В {\bf додатках} зведені детальні результати побудови прогнозів за
допомогою ПРІАМ та їх порівняльний аналіз із результатами отриманими
за допомогою МГВА, НМГВА та РНМ. Також додатки містять скріншоти
роботи розробленого прототипу інформаційної системи та акти
впровадження результатів дисертаційної роботи.

%На рис.~\ref{fig:rcon_report} наведений приклад моделювання динаміки
%реального споживання господарств (РСГ) за даними Держкомстату
%України за 1998-1999~рр.
%\begin{figure}[ht]
%\hbox{%
%\makebox[100mm][c]{\includegraphics[width=100mm]{rcon.pdf}}%
%\makebox[1mm]{}%
%\makebox[64mm][c]{\includegraphics[width=64mm]{rcon_evi.pdf}}%
%}%
%\hbox{
%\makebox[100mm][c]{(a)}\makebox[1mm]{}\makebox[64mm][c]{(в) КБП}%
%}%
%\hbox{%
%\makebox[100mm][c]{\includegraphics[width=100mm]{rcon_res.pdf}}%
%\makebox[1mm]{}%
%\makebox[64mm][c]{\includegraphics[width=64mm]{rcon_gen.pdf}}%
%}%
%\hbox{
%\makebox[100mm][c]{(б)}\makebox[1mm]{}\makebox[64mm][c]{(г) СКПн}%
%}%
%\caption{(a) Дані спостережень (перші 14 точок) і прогноз (останні
%10 точок) на оптимальній ПРІАМ моделі (пунктир) з 95\% довірчим
%інтервалом. Опорні вектори позначені квадратами, вектори в середині
%$\epsilon$-смуги ($\epsilon=0.12$) позначені кругами. (б) Залишки
%різних моделей: неперервна крива --- ПРІАМ, штрих-пунктир
%--- МГВА, пунктир --- РНМ (тільки на інтервалі прогнозу).
%Горизонтальні прямі визначають верхню та нижню межі
%$\epsilon$-смуги. (в, г) Контурні лінії КБП і СКПн в залежності від
%ширини $\epsilon$-смуги (вісь абсцис) для різних значень
%гіперпараметру $\beta$ (вісь ординат). Хрестиками позначені точки
%оптимальних значень гіперпараметрів.} \label{fig:rcon_report}
%\end{figure}
%Динаміка РСГ визначається функцією двох аргументів: ринкової
%відсоткової ставки (РВС --- $x^1$) та реального доходу господарств
%після виплати податків (РДГ --- $x^2$). Вибірка складається з 24
%точок, що відповідає щомісячним спостереженням на протязі двох
%років. Перші 14 точок використовуються для навчання, останні 10 ---
%для прогнозу і обчислення СКПн. Початкова конфігурація моделі
%$\mathbf{C}^{(0)}=\mathrm{diag}\{1,1\}$ виражає апріорні сподівання
%на лінійні залежності. Оптимальна ПРІАМ модель показана на
%рис.~\ref{fig:rcon_report}а. Відповідна оптимальна конфігурація
%$\mathbf{C}_{\mbox{опт}}=\mathrm{diag}\{1,2\}$ посилює ефект від
%РДГ. Розклад опорних векторів в нормованому просторі має вигляд:
%$$
%f(\vect{x})=0.43+6.6k(\vect{x}_{8},\vect{x})+25.5k(\vect{x}_{9},\vect{x})
%-29.8k(\vect{x}_{11},\vect{x}) -2.6k(\vect{x}_{12},\vect{x})
%+0.3k(\vect{x}_{13},\vect{x}),
%$$
%де
%$k(\vect{x}_i,\vect{x}_j)=\vect{\Phi}(\vect{x}_i)\cdot\vect{\Phi}(\vect{x}_j)$.
%Вигляд нейронечіткої моделі у формі Бернштейна:
%$$
%f(\vect{x})=0.43+0.06\phi_0^1(x^1)-0.06\phi_1^1(x^1)
%-0.58\phi_0^2(x^2)+0.13\phi_1^2(x^2)+0.45\phi_2^2(x^2).
%$$
%Більш глибокий аналіз зв'язку між КБП та СКПн за гіперпараметрами
%показано на рис.~\ref{fig:rcon_report}в,
%рис.~\ref{fig:rcon_report}г. Відповідно до контурних ліній
%оптимальна область гіперпараметрів визначається для значень
%$\epsilon$ близьких до 0.12 і для великих значень $\beta$. ПРІАМ
%знаходить відповідно оптимальні значення $\beta=29.8$,
%$\epsilon=0.12$. Контури СКПн також дають найменші значення помилок
%для $\epsilon$ біля 0.12 і таким чином підтверджують ефективність
%КБП.
%
%Нейронечіткі підмоделі в канонічній формі визначаються так:
%$$
%f_1(x^1)=0.49\phi_0^1(x^1)+0.37\phi_1^1(x^1),\quad
%f_2(x^2)=-0.15\phi_0^2(x^2)+0.56\phi_1^2(x^2)+0.88\phi_2^2(x^2).
%$$
%Підмоделі визначають нечіткі множини для змінних входу (РВС, РДГ)
%через базисні поліноми Бернштейна першого $\phi_0^1$, $\phi_1^1$ та
%другого $\phi_0^2$, $\phi_1^2$, $\phi_2^2$ порядків відповідно. Вони
%відповідають нечітким змінним: {\em низька} та {\em висока}
%відсоткова ставка; {\em низькі}, {\em середні}, {\em високі} реальні
%доходи. Визначимо нечіткі множини для змінної виходу (РСГ) через
%чотири B-сплайни першого порядку визначені на вузлах
%$\{-1;~-0.5;~0;~0.5;~1;~1.5\}$, які відповідають нечітким змінним:
%{\em дуже низьке}, {\em низьке}, {\em середнє}, {\em високе} реальне
%споживання з центрами в точках $\{-0.5;~0;~0.5;~1\}$. Тоді,
%розв'язавши дві системи~(\ref{Confidence-systems}) для підмоделі
%$f_1$ і три системи для $f_2$, отримаємо нечітку базу знань з п'яти
%простих правил:
%$$
%\left[
%\begin{array}{l}
%\mbox{\small\sc Правило 1: якщо РВС низька тоді РСГ низьке (0.02) або середнє (0.98)}\\
%\mbox{\small\sc Правило 2: якщо РВС висока тоді РСГ низьке (0.26) або середнє (0.74)}\\
%\mbox{\small\sc Правило 3: якщо РДГ низькі тоді РСГ дуже низьке (0.3) або низьке (0.7)}\\
%\mbox{\small\sc Правило 4: якщо РДГ середні тоді РСГ середнє (0.88) або високе (0.12)}\\
%\mbox{\small\sc Правило 5: якщо РДГ високі тоді РСГ середнє (0.24)
%або високе (0.76)}
%\end{array}\right.
%$$
%На відміну від моделей, які згенеровані за допомогою МГВА, РНМ чи
%навіть ANFIS, цей набір правил є абсолютно прозорим для експертів.
%Експерт легко може визначити степінь впливу того чи іншого фактору.
%Можна також визначити адекватність моделі реальним представленням
%експерта про процес.

\section*{ВИСНОВКИ}

1. Проведено аналіз методів прогнозування стохастичних процесів та
обґрунтована ефективність застосування нечітких баз знань (НБЗ)
побудованих на основі нейронечітких технологій. Встановлено, що
основним недоліком побудованих НБЗ є їх експонентна складність, а
для синтезу НБЗ пониженої складності доцільно використовувати
нейронечіткі моделі у формі Бернштейна.

2. Створена нейронечітка інформаційна технологія синтезу НБЗ
квадратичної складності на основі збалансованих нейронечітких
моделей у формі Бернштейна, де для обґрунтування декомпозиції моделі
у формі Бернштейна на незалежні підмоделі використовується принцип
максимуму ентропії.

3. Розвинуто прискорений метод побудови нейронечітких моделей у
формі Бернштейна, який для визначення барицентричних координат
використовує швидке обернене відображення Кастельжо (ОВК). Швидкодія
побудови моделей при цьому зросла в 3--4 рази. Для синтезу робастних
нейронечітких моделей у формі Бернштейна вперше запропоновано
використовувати еволюційний метод на основі оптимального ОВК.

4. Показано, що байєсівська регресія опорних векторів (БРОВ)
дозволяє будувати моделі стохастичних процесів з домінуючою
випадковою складовою. Головними перевагами БРОВ є робастність оцінок
її параметрів,
%незалежність представлення моделі від розмірності
%вхідного простору,
розв'язання задачі опуклого квадратичного програмування, побудова
довірчих інтервалів, здатність прогнозувати на коротких вибірках. Це
дозволяє робити прогноз в умовах обмеженої кількості даних при
сильній мультиколінеарності і шумі.

5. Для оцінки адекватності моделей БРОВ отримано новий критерій
(критерій байєсівського підтвердження --- КБП), який, на відміну від
відомих критеріїв, не порушує робастності моделей та має просту
аналітичну форму. Ефективність критерію підтверджена аналізом
залежності похибки прогнозу від значення КБП на модельних прикладах.

%на основі наближення маргінальної правдоподібності методом Лапласа

6. На основі БРОВ в характеристичному просторі поліноміальних
функцій Без’є-Бернштейна розроблено новий індуктивний метод побудови
збалансованих робастних нейронечітких моделей у формі Бернштейна
(ПРІАМ), який відрізняється від аналогів підвищеною якістю
прогнозування стохастичних процесів. Метод враховує апріорну
структуру моделі і працює на коротких вибірках. В середньому
точність прогнозу ПРІАМ вища за точність МГВА на 52\%, РНМ --- на
40\%, НМГВА --- на 26\%, ANFIS --- на 10\%.

7. Розроблена архітектура нової інформаційної системи обробки даних
спостережень, яка реалізує запропоновані методи статистичного
аналізу, та створено її прототип. Унікальність системи полягає в її
здатності оперувати в онлайн режимі. Впровадження прототипу
інформаційної системи в Севастопольській гідрометеорологічній
обсерваторії дало можливість побудувати ефективні прогнозуючі
моделі, які використовуються для підтримки прийняття рішень
прогнозистами з метою уточнення прогнозу параметрів хвиль.

%
%1. Виконано аналіз таких основних проблем навчання моделей
%стохастичних процесів як некоректність оберненої задачі відновлення
%залежності, слабка структурованість і непрозорість моделей для
%експертів, домінуюча випадкова складова. Встановлено, що
%байєсівський механізм регуляризації є найефективнішим засобом
%зменшення структурного ризику моделей. А для забезпечення
%структурованості моделей та їх прозорої інтерпретації на мові
%нечіткої логіки доцільно використовувати гібридні нейронечіткі
%технології, які генерують нечіткі бази знань (НБЗ). Основною
%перешкодою при цьому є експонентна складність НБЗ. Для побудови НБЗ
%квадратичної складності можна застосовувати запропоновані автором
%збалансовані нейронечіткі моделі у формі Бернштейна.
%
%2. Показано, що байєсівська регресія опорних векторів (БРОВ)
%дозволяє будувати моделі стохастичних процесів з домінуючою
%випадковою складовою. Головними перевагами БРОВ є робастність оцінок
%її параметрів, незалежність від розмірності вхідного простору,
%розв'язання задачі опуклого квадратичного програмування, можливість
%побудови довірчих інтервалів, здатність прогнозувати на коротких
%вибірках даних. Це дозволяє прогнозувати процеси з великою кількістю
%вхідних змінних в умовах обмеженої кількості даних навіть при
%сильній мультиколінеарності і шумі. Особлива увага при дослідженні
%байєсівського висновку для БРОВ приділяється питанню адекватності
%моделі. Як результат, введено критерій байєсівського підтвердження
%(КБП) для оцінки адекватності БРОВ. Якщо в якості характеристичного
%простору вибрати простір поліноміальних функцій Без'є-Бернштейна, то
%переваги нейронечіткого підходу можна використати в БРОВ. Так,
%розроблений автором індуктивний метод побудови збалансованих
%нейронечітких моделей в формі Бернштейна індукує моделі, які легко
%можна інтерпретувати за допомогою нечітких правил. Метод дозволяє
%задавати апріорну інформацію про структуру моделі і дає оцінку
%точності прогнозу із заданим рівнем довіри.
%
%3.

\clearpage

\section*{СПИСОК ПУБЛІКАЦІЙ ЗА ТЕМОЮ ДИСЕРТАЦІЇ}

\bibliographystyle{../Shared.TEX/gost71u_m}
\bibliography{../Shared.TEX/biblio}

%\clearpage
\section*{АНОТАЦІЇ}
% до 5000, до 1200 знаків : (70 знаків в рядку)-> до 70 рядків, і до 17 рядків

Митник О.Ю. {\bf Інформаційні технології синтезу робастних
нейронечітких моделей стохастичних процесів.}
--- Рукопис.

Дисертація на здобуття наукового ступеня кандидата технічних наук за
спеціальністю 05.13.06 --- Інформаційні технології.
--- Національний технічний університет України ``Київський
політехнічний інститут'', Київ, 2008.

Робота присвячена розробці нових нейронечітких інформаційних
технологій синтезу нечітких баз знань пониженої складності для
прогнозування стохастичних процесів. Введені збалансовані
нейронечіткі моделі у формі Бернштейна, які генерують систему
спрощених нечітких правил квадратичної складності. Досліджені методи
навчання моделей стохастичних процесів як некоректної оберненої
задачі відновлення стохастичних залежностей за даними спостережень.
Розроблено індуктивний метод побудови збалансованих робастних
нейронечітких моделей у формі Бернштейна на основі байєсівської
регресії опорних векторів в характеристичному просторі
поліноміальних функцій Без'є-Бернштейна (ПРІАМ). Розроблена
архітектура та створено прототип інформаційної онлайн системи
обробки статистичних даних, яка реалізує запропонований метод.
Проведено порівняльний аналіз ПРІАМ з нечітким методом групового
врахування аргументів та рекурентними нейронними мережами на
реальних та штучних еталонних моделях, на економічних,
метеорологічних, екологічних моделях. Результати експериментів
доводять ефективність розробленого методу.

Ключові слова: стохастичні процеси, байєсівське навчання, робастні
нейронечіткі моделі, регресія опорних векторів.


\vspace{1mm}

Мытник О.Ю. {\bf Информационные технологии синтеза робастных
нейронечётких моделей стохастических процессов.}
--- Рукопись.

Диссертация на соискание ученой степени кандидата технических наук
по специальности 05.13.06 --- Информационные технологии.  ---
Национальный технический университет Украины ``Киевский
политехнический институт'', Киев, 2008.


Работа посвящена разработке новых нейронечётких информационных
технологий синтеза нечётких баз знаний пониженной сложности для
прогнозирования стохастических процессов. Введены сбалансированные
нейронечёткие модели в форме Бернштейна, которые генерируют систему
упрощённых нечётких правил квадратичной сложности. Исследованы
методы обучения моделей стохастических процессов как некорректной
обратной задачи восстановления стохастических зависимостей по данным
наблюдений. Разработан индуктивный метод построения сбалансированных
робастных нейронечётких моделей в форме Бернштейна на основе
байесовской регрессии опорных векторов в характеристическом
пространстве полиномиальных функций Безье-Бернштейна (ПРИАМ).
Разработана архитектура и построен прототип информационной онлайн
системы обработки статистических данных, которая реализует
предложенный метод. Проведен сравнительный анализ ПРИАМ с нечётким
методом группового учета аргументов и рекуррентными нейронными
сетями на реальных и искусственных эталонных моделях, на
экономических, метеорологических, экологических моделях. Результаты
экспериментов подтверждают эффективность разработанного метода.

Ключевые слова: стохастические процессы, байесовское обучение,
робастные нейронечеткие модели, регрессия опорных векторов.

\vspace{1mm}

Mytnyk O.Yu. {\bf Information technologies for robust neurofuzzy
stochastic process model fusion.}
--- Manuscript.

Thesis for a candidate of technical sciences degree by the
speciality 05.13.06 --- Information technologies. --- National
technical university of Ukraine ``Kiev polytechnic institute'',
Kiev, 2008.

In this paper we investigate the approaches to forecasting of
stochastic processes based on observed data as an ill-posed
stochastic dependence recovery problem. In the first chapter the
regularization methods are studied to pose the problem well. The
model transparency is also at the center of attention. It is well
known that neurofuzzy modeling is an appealing resource for
transparent knowledge representation. The fact that behavior of
neurofuzzy models can be described naturally as a series of
linguistic human-readable rules makes them suitable for expert and
decision support systems.

The generalized neurofuzzy network of C.Harris is considered in the
second chapter as the state-of-the-art approach to construct the
polynomial complexity transparent models. Such drawbacks as
uncertainty of the knots selection, and slowness of the inverse de
Casteljau procedure using back propagation rule are noted. The fast
inverse de Casteljau mapping based on uniform knots layout is
suggested. Thus we develop an algorithm for the fast training of the
neurofuzzy models in Bernstein form based on analytical calculation
of barycentric coordinates. In order to support independent
generation of fuzzy rules based on separate neurofuzzy submodels we
introduce the notion of the balanced neurofuzzy models. It has been
shown that neurofuzzy models in Bernstein form can be balanced.


The last data mining contests have proved the advantage of Bayesian
and support vector methods. Consequently the third chapter is
devoted to study the Bayesian framework for support vector
regression (SVR), where the main complexity is computation of
evidence. Since the evidence is analytically tractable only in the
simplest cases, usually the different approximation techniques are
applied. Among them expectation propagation (EP), variational
methods, Laplace's method (LM), Markov chain Monte Carlo (MCMC).
Although LM is proved recently to be inaccurate comparing to EP and
MCMC we derive Bayesian Evidence Criterion (BEC) for the fast model
comparison, which drastically reduces the computational costs.
Whereas BEC based on LM overestimates evidence and thus can't be
used for direct evidence computation, at the same time BEC succeeds
in model comparison since it overestimates evidence systematically
for all models. In order to overcome the curse of smoothness of the
$\epsilon$-insensitive loss we propose to use the locally smoothed
loss function, which preserves model sparseness and robustness.
Bayesian framework for SVR has the following advantages. Support
vector expansion is independent from input space dimension and takes
away from curse of dimensionality for reconstruction problem. A
unique solution is found after training as solution of a quadratic
programming problem. SVR model possesses robustness and sparseness.
Bayesian framework allows to make error bars estimation. BEC
supplies us with extra fast model search.

Finally we present an inductive method to build balanced robust
neurofuzzy model in Bernstein form based on Bayesian SVR in feature
space spanned by B\'{e}zier-Bernstein polynomial functions (PRIAM
--- Polynomial Regression Inductive Algorithm Modeling) for
stochastic dependence recovery problem. It combines the precedence
of Bayesian inference, robustness of the support vector approach and
transparency of the high end neurofuzzy modeling. Dual model
conception allows PRIAM both be competitive with modern machine
learning algorithms and be convenient for knowledge representation
in expert systems. The complexity and convergence properties of
PRIAM have been analysed.

We conduct experiments on synthetic reference data sets as well as
on real world economic, ecologic, meteorologic models and compare
PRIAM forecasts with results of group method of data handling
(GMDH), fuzzy GMDH, recurrent neural networks and ANFIS. Our
experiments show that PRIAM outperforms the methods listed above
having parsimony model construction logic.

Keywords: stochastic processes, Bayesian learning, robust neurofuzzy
models, support vector regression.

\end{document}
% ------------------------------------------------------------------------
